functionality:
  name: "censor_dataset"
  namespace: "match_modality_datasets"
  version: "main_build"
  authors:
  - name: "Louise Deconinck"
    email: "louise.deconinck@gmail.com"
    roles:
    - "author"
    - "maintainer"
    props:
      github: "LouiseDck"
      orcid: ""
  - name: "Robrecht Cannoodt"
    email: "rcannood@gmail.com"
    roles:
    - "contributor"
    props:
      github: "rcannood"
      orcid: "0000-0003-3641-729X"
  arguments:
  - type: "file"
    name: "--input_mod1"
    alternatives: []
    description: "The first modality of the input dataset to be censored"
    example: "dataset.h5ad"
    must_exist: false
    required: true
    direction: "Input"
    multiple: false
    multiple_sep: ":"
  - type: "file"
    name: "--input_mod2"
    alternatives: []
    description: "The second modality of the input dataset to be censored"
    example: "dataset.h5ad"
    must_exist: false
    required: true
    direction: "Input"
    multiple: false
    multiple_sep: ":"
  - type: "file"
    name: "--output_train_mod1"
    alternatives: []
    description: "The censored shuffled train mod1 profiles."
    example: "dataset_censored.h5ad"
    must_exist: false
    required: true
    direction: "Output"
    multiple: false
    multiple_sep: ":"
  - type: "file"
    name: "--output_train_mod2"
    alternatives: []
    description: "The censored shuffled train mod2 profiles."
    example: "dataset_censored.h5ad"
    must_exist: false
    required: true
    direction: "Output"
    multiple: false
    multiple_sep: ":"
  - type: "file"
    name: "--output_train_sol"
    alternatives: []
    description: "The pairing of train mod1&mod2 profiles."
    example: "dataset_solution.h5ad"
    must_exist: false
    required: true
    direction: "Output"
    multiple: false
    multiple_sep: ":"
  - type: "file"
    name: "--output_test_mod1"
    alternatives: []
    description: "The censored shuffled test mod1 profiles."
    example: "dataset_censored.h5ad"
    must_exist: false
    required: true
    direction: "Output"
    multiple: false
    multiple_sep: ":"
  - type: "file"
    name: "--output_test_mod2"
    alternatives: []
    description: "The censored shuffled test mod2 profiles."
    example: "dataset_censored.h5ad"
    must_exist: false
    required: true
    direction: "Output"
    multiple: false
    multiple_sep: ":"
  - type: "file"
    name: "--output_test_sol"
    alternatives: []
    description: "The pairing of test mod1&mod2 profiles."
    example: "dataset_solution.h5ad"
    must_exist: false
    required: true
    direction: "Output"
    multiple: false
    multiple_sep: ":"
  - type: "integer"
    name: "--knn"
    alternatives: []
    description: "The KNN cutoff for computing nearest neighbors on the mod1 and mod2\
      \ profiles."
    default: 20
    required: false
    direction: "Input"
    multiple: false
    multiple_sep: ":"
  - type: "integer"
    name: "--seed"
    alternatives: []
    description: "The seed."
    default: 1
    required: false
    direction: "Input"
    multiple: false
    multiple_sep: ":"
  resources:
  - type: "bash_script"
    text: |
      #!/usr/bin/env bash
      
      ###################################
      #    censor_dataset main_build    #
      ###################################
      
      # This wrapper script is auto-generated by viash 0.5.3 and is thus a derivative
      # work thereof. This software comes with ABSOLUTELY NO WARRANTY from Data
      # Intuitive.  The component may contain files which fall under a different
      # license. The authors of this component should specify the license in the
      # header of such files, or include a separate license file detailing the
      # licenses of all included files.
      #
      # Component authors:
      # * Louise Deconinck <louise.deconinck@gmail.com> (author, maintainer) {github: LouiseDck, orcid: }
      # * Robrecht Cannoodt <rcannood@gmail.com> (contributor) {github: rcannood, orcid: 0000-0003-3641-729X}
      
      set -e
      
      if [ -z "$VIASH_TEMP" ]; then
        VIASH_TEMP=/tmp
      fi
      
      # define helper functions
      # ViashQuote: put quotes around non flag values
      # $1     : unquoted string
      # return : possibly quoted string
      # examples:
      #   ViashQuote --foo      # returns --foo
      #   ViashQuote bar        # returns 'bar'
      #   Viashquote --foo=bar  # returns --foo='bar'
      function ViashQuote {
        if [[ "$1" =~ ^-+[a-zA-Z0-9_\-]+=.+$ ]]; then
          echo "$1" | sed "s#=\(.*\)#='\1'#"
        elif [[ "$1" =~ ^-+[a-zA-Z0-9_\-]+$ ]]; then
          echo "$1"
        else
          echo "'$1'"
        fi
      }
      # ViashRemoveFlags: Remove leading flag
      # $1     : string with a possible leading flag
      # return : string without possible leading flag
      # examples:
      #   ViashRemoveFlags --foo=bar  # returns bar
      function ViashRemoveFlags {
        echo "$1" | sed 's/^--*[a-zA-Z0-9_\-]*=//'
      }
      # ViashSourceDir: return the path of a bash file, following symlinks
      # usage   : ViashSourceDir ${BASH_SOURCE[0]}
      # $1      : Should always be set to ${BASH_SOURCE[0]}
      # returns : The absolute path of the bash file
      function ViashSourceDir {
        SOURCE="$1"
        while [ -h "$SOURCE" ]; do
          DIR="$( cd -P "$( dirname "$SOURCE" )" >/dev/null 2>&1 && pwd )"
          SOURCE="$(readlink "$SOURCE")"
          [[ $SOURCE != /* ]] && SOURCE="$DIR/$SOURCE"
        done
        cd -P "$( dirname "$SOURCE" )" >/dev/null 2>&1 && pwd
      }
      VIASH_VERBOSITY=5
      
      # see https://en.wikipedia.org/wiki/Syslog#Severity_level
      
      # ViashLog: Log events depending on the verbosity level
      # usage: ViashLog 1 alert Oh no something went wrong!
      # $1: required verbosity level
      # $2: display tag
      # $3+: messages to display
      # stdout: Your input, prepended by '[$2] '. 
      function ViashLog {
        local required_level="$1"
        local display_tag="$2"
        shift 2
        if [ $VIASH_VERBOSITY -ge $required_level ]; then
          echo "[$display_tag]" "$@"
        fi
      }
      
      # ViashEmergency: log events when the system is unstable
      # usage: ViashEmergency Oh no something went wrong.
      # stdout: Your input, prepended by '[emergency] '. 
      function ViashEmergency {
        ViashLog 0 emergency $@
      }
      
      # ViashAlert: log events when actions must be taken immediately (e.g. corrupted system database)
      # usage: ViashAlert Oh no something went wrong.
      # stdout: Your input, prepended by '[alert] '. 
      function ViashAlert {
        ViashLog 1 alert $@
      }
      
      # ViashCritical: log events when a critical condition occurs
      # usage: ViashCritical Oh no something went wrong.
      # stdout: Your input, prepended by '[critical] '. 
      function ViashCritical {
        ViashLog 2 critical $@
      }
      
      # ViashError: log events when an error condition occurs
      # usage: ViashError Oh no something went wrong.
      # stdout: Your input, prepended by '[error] '. 
      function ViashError {
        ViashLog 3 error $@
      }
      
      # ViashWarning: log potentially abnormal events
      # usage: ViashWarning Something may have gone wrong.
      # stdout: Your input, prepended by '[warning] '. 
      function ViashWarning {
        ViashLog 4 warning $@
      }
      
      # ViashNotice: log significant but normal events
      # usage: ViashNotice This just happened.
      # stdout: Your input, prepended by '[notice] '. 
      function ViashNotice {
        ViashLog 5 notice $@
      }
      
      # ViashInfo: log normal events
      # usage: ViashInfo This just happened.
      # stdout: Your input, prepended by '[info] '. 
      function ViashInfo {
        ViashLog 6 info $@
      }
      
      # ViashDebug: log all events, for debugging purposes
      # usage: ViashDebug This just happened.
      # stdout: Your input, prepended by '[debug] '. 
      function ViashDebug {
        ViashLog 7 debug $@
      }
      
      # find source folder of this component
      VIASH_RESOURCES_DIR=`ViashSourceDir ${BASH_SOURCE[0]}`
      
      # define meta fields
      VIASH_META_FUNCTIONALITY_NAME="censor_dataset"
      
      
      # ViashHelp: Display helpful explanation about this executable
      function ViashHelp {
        echo "censor_dataset main_build"
        echo "Censor an existing dataset: obfuscate gene names, remove cell identities and"
        echo "shuffle cells of modalities, for distribution to competitors."
        echo
        echo "Options:"
      
        echo "   --input_mod1"
        echo "        type: file, required parameter"
        echo "        example: dataset.h5ad"
        echo "        The first modality of the input dataset to be censored"
        echo ""
      
      
        echo "   --input_mod2"
        echo "        type: file, required parameter"
        echo "        example: dataset.h5ad"
        echo "        The second modality of the input dataset to be censored"
        echo ""
      
      
        echo "   --output_train_mod1"
        echo "        type: file, required parameter, output"
        echo "        example: dataset_censored.h5ad"
        echo "        The censored shuffled train mod1 profiles."
        echo ""
      
      
        echo "   --output_train_mod2"
        echo "        type: file, required parameter, output"
        echo "        example: dataset_censored.h5ad"
        echo "        The censored shuffled train mod2 profiles."
        echo ""
      
      
        echo "   --output_train_sol"
        echo "        type: file, required parameter, output"
        echo "        example: dataset_solution.h5ad"
        echo "        The pairing of train mod1&mod2 profiles."
        echo ""
      
      
        echo "   --output_test_mod1"
        echo "        type: file, required parameter, output"
        echo "        example: dataset_censored.h5ad"
        echo "        The censored shuffled test mod1 profiles."
        echo ""
      
      
        echo "   --output_test_mod2"
        echo "        type: file, required parameter, output"
        echo "        example: dataset_censored.h5ad"
        echo "        The censored shuffled test mod2 profiles."
        echo ""
      
      
        echo "   --output_test_sol"
        echo "        type: file, required parameter, output"
        echo "        example: dataset_solution.h5ad"
        echo "        The pairing of test mod1&mod2 profiles."
        echo ""
      
      
        echo "   --knn"
        echo "        type: integer"
        echo "        default: 20"
        echo "        The KNN cutoff for computing nearest neighbors on the mod1 and mod2 profiles."
        echo ""
      
      
        echo "   --seed"
        echo "        type: integer"
        echo "        default: 1"
        echo "        The seed."
        echo ""
      
      }
      
      # initialise array
      VIASH_POSITIONAL_ARGS=''
      
      while [[ $# -gt 0 ]]; do
          case "$1" in
              -h|--help)
                  ViashHelp
                  exit
                  ;;
              -v|--verbose)
                  let "VIASH_VERBOSITY=VIASH_VERBOSITY+1"
                  shift 1
                  ;;
              -vv)
                  let "VIASH_VERBOSITY=VIASH_VERBOSITY+2"
                  shift 1
                  ;;
              --verbosity)
                  VIASH_VERBOSITY="$2"
                  shift 2
                  ;;
              --verbosity=*)
                  VIASH_VERBOSITY="$(ViashRemoveFlags "$1")"
                  shift 1
                  ;;
              --version)
                  echo "censor_dataset main_build"
                  exit
                  ;;
              --input_mod1)
                  VIASH_PAR_INPUT_MOD1="$2"
                  shift 2
                  ;;
              --input_mod1=*)
                  VIASH_PAR_INPUT_MOD1=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              --input_mod2)
                  VIASH_PAR_INPUT_MOD2="$2"
                  shift 2
                  ;;
              --input_mod2=*)
                  VIASH_PAR_INPUT_MOD2=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              --output_train_mod1)
                  VIASH_PAR_OUTPUT_TRAIN_MOD1="$2"
                  shift 2
                  ;;
              --output_train_mod1=*)
                  VIASH_PAR_OUTPUT_TRAIN_MOD1=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              --output_train_mod2)
                  VIASH_PAR_OUTPUT_TRAIN_MOD2="$2"
                  shift 2
                  ;;
              --output_train_mod2=*)
                  VIASH_PAR_OUTPUT_TRAIN_MOD2=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              --output_train_sol)
                  VIASH_PAR_OUTPUT_TRAIN_SOL="$2"
                  shift 2
                  ;;
              --output_train_sol=*)
                  VIASH_PAR_OUTPUT_TRAIN_SOL=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              --output_test_mod1)
                  VIASH_PAR_OUTPUT_TEST_MOD1="$2"
                  shift 2
                  ;;
              --output_test_mod1=*)
                  VIASH_PAR_OUTPUT_TEST_MOD1=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              --output_test_mod2)
                  VIASH_PAR_OUTPUT_TEST_MOD2="$2"
                  shift 2
                  ;;
              --output_test_mod2=*)
                  VIASH_PAR_OUTPUT_TEST_MOD2=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              --output_test_sol)
                  VIASH_PAR_OUTPUT_TEST_SOL="$2"
                  shift 2
                  ;;
              --output_test_sol=*)
                  VIASH_PAR_OUTPUT_TEST_SOL=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              --knn)
                  VIASH_PAR_KNN="$2"
                  shift 2
                  ;;
              --knn=*)
                  VIASH_PAR_KNN=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              --seed)
                  VIASH_PAR_SEED="$2"
                  shift 2
                  ;;
              --seed=*)
                  VIASH_PAR_SEED=$(ViashRemoveFlags "$1")
                  shift 1
                  ;;
              *)  # positional arg or unknown option
                  # since the positional args will be eval'd, can we always quote, instead of using ViashQuote
                  VIASH_POSITIONAL_ARGS="$VIASH_POSITIONAL_ARGS '$1'"
                  shift # past argument
                  ;;
          esac
      done
      
      # parse positional parameters
      eval set -- $VIASH_POSITIONAL_ARGS
      
      
      
      # check whether required parameters exist
      if [ -z "$VIASH_PAR_INPUT_MOD1" ]; then
        ViashError '--input_mod1' is a required argument. Use "--help" to get more information on the parameters.
        exit 1
      fi
      if [ -z "$VIASH_PAR_INPUT_MOD2" ]; then
        ViashError '--input_mod2' is a required argument. Use "--help" to get more information on the parameters.
        exit 1
      fi
      if [ -z "$VIASH_PAR_OUTPUT_TRAIN_MOD1" ]; then
        ViashError '--output_train_mod1' is a required argument. Use "--help" to get more information on the parameters.
        exit 1
      fi
      if [ -z "$VIASH_PAR_OUTPUT_TRAIN_MOD2" ]; then
        ViashError '--output_train_mod2' is a required argument. Use "--help" to get more information on the parameters.
        exit 1
      fi
      if [ -z "$VIASH_PAR_OUTPUT_TRAIN_SOL" ]; then
        ViashError '--output_train_sol' is a required argument. Use "--help" to get more information on the parameters.
        exit 1
      fi
      if [ -z "$VIASH_PAR_OUTPUT_TEST_MOD1" ]; then
        ViashError '--output_test_mod1' is a required argument. Use "--help" to get more information on the parameters.
        exit 1
      fi
      if [ -z "$VIASH_PAR_OUTPUT_TEST_MOD2" ]; then
        ViashError '--output_test_mod2' is a required argument. Use "--help" to get more information on the parameters.
        exit 1
      fi
      if [ -z "$VIASH_PAR_OUTPUT_TEST_SOL" ]; then
        ViashError '--output_test_sol' is a required argument. Use "--help" to get more information on the parameters.
        exit 1
      fi
      if [ -z "$VIASH_PAR_KNN" ]; then
        VIASH_PAR_KNN="20"
      fi
      if [ -z "$VIASH_PAR_SEED" ]; then
        VIASH_PAR_SEED="1"
      fi
      
      
      cat << VIASHEOF | bash
      set -e
      tempscript=\$(mktemp "$VIASH_TEMP/viash-run-censor_dataset-XXXXXX")
      function clean_up {
        rm "\$tempscript"
      }
      trap clean_up EXIT
      cat > "\$tempscript" << 'VIASHMAIN'
      cat("Loading dependencies\\n")
      options(tidyverse.quiet = TRUE)
      library(tidyverse)
      requireNamespace("anndata", quietly = TRUE)
      library(assertthat, quietly = TRUE, warn.conflicts = FALSE)
      library(Matrix, quietly = TRUE, warn.conflicts = FALSE)
      
      ## VIASH START
      # The following code has been auto-generated by Viash.
      # get parameters from cli
      par <- list(
        "input_mod1" = $( if [ ! -z ${VIASH_PAR_INPUT_MOD1+x} ]; then echo "'$VIASH_PAR_INPUT_MOD1'"; else echo NULL; fi ),
        "input_mod2" = $( if [ ! -z ${VIASH_PAR_INPUT_MOD2+x} ]; then echo "'$VIASH_PAR_INPUT_MOD2'"; else echo NULL; fi ),
        "output_train_mod1" = $( if [ ! -z ${VIASH_PAR_OUTPUT_TRAIN_MOD1+x} ]; then echo "'$VIASH_PAR_OUTPUT_TRAIN_MOD1'"; else echo NULL; fi ),
        "output_train_mod2" = $( if [ ! -z ${VIASH_PAR_OUTPUT_TRAIN_MOD2+x} ]; then echo "'$VIASH_PAR_OUTPUT_TRAIN_MOD2'"; else echo NULL; fi ),
        "output_train_sol" = $( if [ ! -z ${VIASH_PAR_OUTPUT_TRAIN_SOL+x} ]; then echo "'$VIASH_PAR_OUTPUT_TRAIN_SOL'"; else echo NULL; fi ),
        "output_test_mod1" = $( if [ ! -z ${VIASH_PAR_OUTPUT_TEST_MOD1+x} ]; then echo "'$VIASH_PAR_OUTPUT_TEST_MOD1'"; else echo NULL; fi ),
        "output_test_mod2" = $( if [ ! -z ${VIASH_PAR_OUTPUT_TEST_MOD2+x} ]; then echo "'$VIASH_PAR_OUTPUT_TEST_MOD2'"; else echo NULL; fi ),
        "output_test_sol" = $( if [ ! -z ${VIASH_PAR_OUTPUT_TEST_SOL+x} ]; then echo "'$VIASH_PAR_OUTPUT_TEST_SOL'"; else echo NULL; fi ),
        "knn" = $( if [ ! -z ${VIASH_PAR_KNN+x} ]; then echo "as.integer($VIASH_PAR_KNN)"; else echo NULL; fi ),
        "seed" = $( if [ ! -z ${VIASH_PAR_SEED+x} ]; then echo "as.integer($VIASH_PAR_SEED)"; else echo NULL; fi )
      )
      
      # get meta parameters
      meta <- list(
        functionality_name = "$VIASH_META_FUNCTIONALITY_NAME",
        resources_dir = "$VIASH_RESOURCES_DIR"
      )
      
      # get resources dir
      resources_dir = "$VIASH_RESOURCES_DIR"
      
      ## VIASH END
      
      set.seed(par\$seed)
      
      cat("Reading input data\\n")
      input_mod1 <- anndata::read_h5ad(par\$input_mod1)
      input_mod2 <- anndata::read_h5ad(par\$input_mod2)
      ad1_mod <- unique(input_mod1\$var[["feature_types"]])
      ad2_mod <- unique(input_mod2\$var[["feature_types"]])
      new_dataset_id <- paste0(input_mod1\$uns[["dataset_id"]], "_MM_", tolower(ad1_mod), "2", tolower(ad2_mod))
      common_uns <- list(dataset_id = new_dataset_id)
      
      cat("Shuffle train cells\\n")
      train_ix <- which(input_mod1\$obs\$is_train) %>% sort
      train_mod2_ix <- sample.int(length(train_ix))
      
      cat("Shuffle test cells\\n")
      test_ix <- which(!input_mod1\$obs\$is_train) %>% sort
      test_mod2_ix <- sample.int(length(test_ix))
      
      is_categorical <- function(x) is.character(x) || is.factor(x)
      # relevel <- function(x) factor(as.character(x))
      relevel <- function(x) as.character(x)
      
      cat("Creating train objects\\n")
      mod1_var <- input_mod1\$var %>% select(one_of("gene_ids", "feature_types"))
      mod2_var <- input_mod2\$var %>% select(one_of("gene_ids", "feature_types"))
      train_obs1 <- input_mod1\$obs[train_ix, , drop = FALSE] %>% 
        select(one_of("batch", "size_factors")) %>% 
        mutate_if(is_categorical, relevel)
      train_obs2 <- input_mod2\$obs[train_ix, , drop = FALSE] %>% 
        select(one_of("size_factors")) %>% 
        mutate_if(is_categorical, relevel)
      rownames(train_obs2) <- NULL
      if (ncol(train_obs2) == 0) train_obs2 <- NULL
      
      output_train_mod1 <- anndata::AnnData(
        X = input_mod1\$X[train_ix, , drop = FALSE],
        layers = list(counts = input_mod1\$layers[["counts"]][train_ix, , drop = FALSE]),
        obs = train_obs1,
        var = mod1_var,
        uns = common_uns
      )
      output_train_mod2 <- anndata::AnnData(
        X = input_mod2\$X[train_ix[train_mod2_ix], , drop = FALSE] %>%
          magrittr::set_rownames(., paste0("cell_", seq_len(nrow(.)))),
        layers = list(counts = input_mod2\$layers[["counts"]][train_ix[train_mod2_ix], , drop = FALSE] %>%
          magrittr::set_rownames(., paste0("cell_", seq_len(nrow(.))))),
        obs = train_obs2,
        var = mod2_var,
        uns = common_uns
      )
      
      cat("Create test objects\\n")
      test_obs1 <- input_mod1\$obs[test_ix, , drop = FALSE] %>% 
        select(one_of("batch", "size_factors")) %>% 
        mutate_if(is_categorical, relevel)
      test_obs2 <- input_mod1\$obs[test_ix, , drop = FALSE] %>% 
        select(one_of("size_factors")) %>% 
        mutate_if(is_categorical, relevel)
      rownames(test_obs2) <- NULL
      if (ncol(test_obs2) == 0) test_obs2 <- NULL
      
      output_test_mod1 <- anndata::AnnData(
        X = input_mod1\$X[test_ix, , drop = FALSE],
        layers = list(counts = input_mod1\$layers[["counts"]][test_ix, , drop = FALSE]),
        obs = test_obs1,
        var = mod1_var,
        uns = common_uns
      )
      output_test_mod2 <- anndata::AnnData(
        X = input_mod2\$X[test_ix[test_mod2_ix], , drop = FALSE] %>% 
          magrittr::set_rownames(., paste0("cell_", seq_len(nrow(.)))),
        layers = list(counts = input_mod2\$layers[["counts"]][test_ix[test_mod2_ix], , drop = FALSE] %>% 
          magrittr::set_rownames(., paste0("cell_", seq_len(nrow(.))))),
        obs = test_obs2,
        var = mod2_var,
        uns = common_uns
      )
      
      cat("Create solution objects\\n")
      
      train_sol_mat <- Matrix::sparseMatrix(
        i = seq_along(train_mod2_ix),
        j = order(train_mod2_ix),
        x = rep(1, length(train_mod2_ix))
      )
      output_train_sol <- anndata::AnnData(
        X = train_sol_mat,
        obs = input_mod1\$obs[train_ix, , drop = FALSE] %>% select(one_of(c("batch"))) %>% mutate_if(is_categorical, relevel),
        uns = list(dataset_id = new_dataset_id, pairing_ix = train_mod2_ix - 1)
      )
      
      test_sol_mat <- Matrix::sparseMatrix(
        i = seq_along(test_mod2_ix),
        j = order(test_mod2_ix),
        x = rep(1, length(test_mod2_ix))
      )
      output_test_sol <- anndata::AnnData(
        X = test_sol_mat,
        obs = input_mod1\$obs[test_ix, , drop = FALSE] %>% select(one_of(c("batch"))) %>% mutate_if(is_categorical, relevel),
        uns = list(dataset_id = new_dataset_id, pairing_ix = test_mod2_ix - 1)
      )
      
      # checks
      # mean(rowSums(train_solknn > 0))
      # mean(rowSums(test_solknn > 0))
      # sum(train_solknn * train_sol_mat) == nrow(train_sol_mat)
      # sum(test_solknn * test_sol_mat) == nrow(test_sol_mat)
      
      cat("Saving output files as h5ad\\n")
      zzz <- output_train_mod1\$write_h5ad(par\$output_train_mod1, compression = "gzip")
      zzz <- output_train_mod2\$write_h5ad(par\$output_train_mod2, compression = "gzip")
      zzz <- output_train_sol\$write_h5ad(par\$output_train_sol, compression = "gzip")
      zzz <- output_test_mod1\$write_h5ad(par\$output_test_mod1, compression = "gzip")
      zzz <- output_test_mod2\$write_h5ad(par\$output_test_mod2, compression = "gzip")
      zzz <- output_test_sol\$write_h5ad(par\$output_test_sol, compression = "gzip")
      VIASHMAIN
      PATH="$VIASH_RESOURCES_DIR:\$PATH"
      
      Rscript "\$tempscript"
      
      VIASHEOF
      

    dest: "censor_dataset"
    is_executable: true
  - type: "file"
    text: |
        docker.enabled = true
        def viash_temp = System.getenv("VIASH_TEMP") ?: "/tmp/"
        docker.runOptions = "-i -v ${baseDir}:${baseDir} -v $viash_temp:$viash_temp"
        process.container = "dataintuitive/viash"
        params {
          censor_dataset__input_mod1 = "viash_no_value"
          censor_dataset__input_mod2 = "viash_no_value"
          censor_dataset__output_train_mod1 = "viash_no_value"
          censor_dataset__output_train_mod2 = "viash_no_value"
          censor_dataset__output_train_sol = "viash_no_value"
          censor_dataset__output_test_mod1 = "viash_no_value"
          censor_dataset__output_test_mod2 = "viash_no_value"
          censor_dataset__output_test_sol = "viash_no_value"
          censor_dataset__knn = "20"
          censor_dataset__seed = "1"
          id = ""
          testScript = ""
          testResources = [  ]
          censor_dataset {
            name = "censor_dataset"
            container = "match_modality_datasets_censor_dataset"
            containerTag = "main_build"
            containerRegistry = "openproblems"
            command = "censor_dataset"
            tests {
              isDefined = false
              testScript = "NA"
              testResources = [  ]
            }
            arguments {
              input_mod1 {
                name = "input_mod1"
                otype = "--"
                required = true
                type = "file"
                direction = "Input"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__input_mod1}"
                example = "dataset.h5ad"
                description = "The first modality of the input dataset to be censored"
              }
              input_mod2 {
                name = "input_mod2"
                otype = "--"
                required = true
                type = "file"
                direction = "Input"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__input_mod2}"
                example = "dataset.h5ad"
                description = "The second modality of the input dataset to be censored"
              }
              output_train_mod1 {
                name = "output_train_mod1"
                otype = "--"
                required = true
                type = "file"
                direction = "Output"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__output_train_mod1}"
                example = "dataset_censored.h5ad"
                description = "The censored shuffled train mod1 profiles."
              }
              output_train_mod2 {
                name = "output_train_mod2"
                otype = "--"
                required = true
                type = "file"
                direction = "Output"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__output_train_mod2}"
                example = "dataset_censored.h5ad"
                description = "The censored shuffled train mod2 profiles."
              }
              output_train_sol {
                name = "output_train_sol"
                otype = "--"
                required = true
                type = "file"
                direction = "Output"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__output_train_sol}"
                example = "dataset_solution.h5ad"
                description = "The pairing of train mod1&mod2 profiles."
              }
              output_test_mod1 {
                name = "output_test_mod1"
                otype = "--"
                required = true
                type = "file"
                direction = "Output"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__output_test_mod1}"
                example = "dataset_censored.h5ad"
                description = "The censored shuffled test mod1 profiles."
              }
              output_test_mod2 {
                name = "output_test_mod2"
                otype = "--"
                required = true
                type = "file"
                direction = "Output"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__output_test_mod2}"
                example = "dataset_censored.h5ad"
                description = "The censored shuffled test mod2 profiles."
              }
              output_test_sol {
                name = "output_test_sol"
                otype = "--"
                required = true
                type = "file"
                direction = "Output"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__output_test_sol}"
                example = "dataset_solution.h5ad"
                description = "The pairing of test mod1&mod2 profiles."
              }
              knn {
                name = "knn"
                otype = "--"
                required = false
                type = "integer"
                direction = "Input"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__knn}"
                dflt = "20"
                description = "The KNN cutoff for computing nearest neighbors on the mod1 and mod2 profiles."
              }
              seed {
                name = "seed"
                otype = "--"
                required = false
                type = "integer"
                direction = "Input"
                multiple = false
                multiple_sep = ":"
                value = "${params.censor_dataset__seed}"
                dflt = "1"
                description = "The seed."
              }
            }
          }
        }

    dest: "nextflow.config"
  - type: "file"
    text: |
      nextflow.enable.dsl=2
      
      params.test = false
      params.debug = false
      params.publishDir = "./"
      
      // A function to verify (at runtime) if all required arguments are effectively provided.
      def checkParams(_params) {
        _params.arguments.collect{
          if (it.value == "viash_no_value") {
            println("[ERROR] option --${it.name} not specified in component censor_dataset")
            println("exiting now...")
              exit 1
          }
        }
      }
      
      
      def escape(str) {
        return str.replaceAll('\\\\', '\\\\\\\\').replaceAll("\"", "\\\\\"").replaceAll("\n", "\\\\n").replaceAll("`", "\\\\`")
      }
      
      def renderArg(it) {
        if (it.otype == "") {
          return "'" + escape(it.value) + "'"
        } else if (it.type == "boolean_true") {
          if (it.value.toLowerCase() == "true") {
            return it.otype + it.name
          } else {
            return ""
          }
        } else if (it.type == "boolean_false") {
          if (it.value.toLowerCase() == "true") {
            return ""
          } else {
            return it.otype + it.name
          }
        } else if (it.value == "no_default_value_configured") {
          return ""
        } else {
          def retVal = it.value in List && it.multiple ? it.value.join(it.multiple_sep): it.value
          return it.otype + it.name + " '" + escape(retVal) + "'"
        }
      }
      
      def renderCLI(command, arguments) {
        def argumentsList = arguments.collect{renderArg(it)}.findAll{it != ""}
      
        def command_line = command + argumentsList
      
        return command_line.join(" ")
      }
      
      def effectiveContainer(processParams) {
        def _registry = params.containsKey("containerRegistry") ? params.containerRegistry : processParams.containerRegistry
        def _name = processParams.container
        def _tag = params.containsKey("containerTag") ? params.containerTag : processParams.containerTag
      
        return (_registry == "" ? "" : _registry + "/") + _name + ":" + _tag
      }
      
      // Convert the nextflow.config arguments list to a List instead of a LinkedHashMap
      // The rest of this main.nf script uses the Map form
      def argumentsAsList(_params) {
        def overrideArgs = _params.arguments.collect{ key, value -> value }
        def newParams = _params + [ "arguments" : overrideArgs ]
        return newParams
      }
      
      
      // Use the params map, create a hashmap of the filenames for output
      // output filename is <sample>.<method>.<arg_name>[.extension]
      def outFromIn(_params) {
      
        def id = _params.id
      
        _params
          .arguments
          .findAll{ it -> it.type == "file" && it.direction == "Output" }
          .collect{ it ->
            // If an 'example' attribute is present, strip the extension from the filename,
            // If a 'dflt' attribute is present, strip the extension from the filename,
            // Otherwise just use the option name as an extension.
            def extOrName =
              (it.example != null)
                ? it.example.split(/\./).last()
                : (it.dflt != null)
                  ? it.dflt.split(/\./).last()
                  : it.name
            // The output filename is <sample> . <modulename> . <extension>
            // Unless the output argument is explicitly specified on the CLI
            def newValue =
              (it.value == "viash_no_value")
                ? "censor_dataset." + it.name + "." + extOrName
                : it.value
            def newName =
              (id != "")
                ? id + "." + newValue
                : it.name + newValue
            it + [ value : newName ]
          }
      
      }
      
      // A process that filters out output_train_mod1 from the output Map
      process filterOutput_train_mod1 {
      
        input:
          tuple val(id), val(input), val(_params)
        output:
          tuple val(id), val(output), val(_params)
        when:
          input.keySet().contains("output_train_mod1")
        exec:
          output = input["output_train_mod1"]
      
      }
      
      // A process that filters out output_train_mod2 from the output Map
      process filterOutput_train_mod2 {
      
        input:
          tuple val(id), val(input), val(_params)
        output:
          tuple val(id), val(output), val(_params)
        when:
          input.keySet().contains("output_train_mod2")
        exec:
          output = input["output_train_mod2"]
      
      }
      
      // A process that filters out output_train_sol from the output Map
      process filterOutput_train_sol {
      
        input:
          tuple val(id), val(input), val(_params)
        output:
          tuple val(id), val(output), val(_params)
        when:
          input.keySet().contains("output_train_sol")
        exec:
          output = input["output_train_sol"]
      
      }
      
      // A process that filters out output_test_mod1 from the output Map
      process filterOutput_test_mod1 {
      
        input:
          tuple val(id), val(input), val(_params)
        output:
          tuple val(id), val(output), val(_params)
        when:
          input.keySet().contains("output_test_mod1")
        exec:
          output = input["output_test_mod1"]
      
      }
      
      // A process that filters out output_test_mod2 from the output Map
      process filterOutput_test_mod2 {
      
        input:
          tuple val(id), val(input), val(_params)
        output:
          tuple val(id), val(output), val(_params)
        when:
          input.keySet().contains("output_test_mod2")
        exec:
          output = input["output_test_mod2"]
      
      }
      
      // A process that filters out output_test_sol from the output Map
      process filterOutput_test_sol {
      
        input:
          tuple val(id), val(input), val(_params)
        output:
          tuple val(id), val(output), val(_params)
        when:
          input.keySet().contains("output_test_sol")
        exec:
          output = input["output_test_sol"]
      
      }
      
      def overrideIO(_params, inputs, outputs) {
      
        // `inputs` in fact can be one of:
        // - `String`,
        // - `List[String]`,
        // - `Map[String, String | List[String]]`
        // Please refer to the docs for more info
        def overrideArgs = _params.arguments.collect{ it ->
          if (it.type == "file") {
            if (it.direction == "Input") {
              (inputs in List || inputs in HashMap)
                ? (inputs in List)
                  ? it + [ "value" : inputs.join(it.multiple_sep)]
                  : (inputs[it.name] != null)
                    ? (inputs[it.name] in List)
                      ? it + [ "value" : inputs[it.name].join(it.multiple_sep)]
                      : it + [ "value" : inputs[it.name]]
                    : it
                : it + [ "value" : inputs ]
            } else {
              (outputs in List || outputs in HashMap)
                ? (outputs in List)
                  ? it + [ "value" : outputs.join(it.multiple_sep)]
                  : (outputs[it.name] != null)
                    ? (outputs[it.name] in List)
                      ? it + [ "value" : outputs[it.name].join(it.multiple_sep)]
                      : it + [ "value" : outputs[it.name]]
                    : it
                : it + [ "value" : outputs ]
            }
          } else {
            it
          }
        }
      
        def newParams = _params + [ "arguments" : overrideArgs ]
      
        return newParams
      
      }
      
      process censor_dataset_process {
        label 'highmem'
        label 'midtime'
        label 'highcpu'
        tag "${id}"
        echo { (params.debug == true) ? true : false }
        cache 'deep'
        stageInMode "symlink"
        container "${container}"
        publishDir "${params.publishDir}/${id}/", mode: 'copy', overwrite: true, enabled: !params.test
        input:
          tuple val(id), path(input), val(output), val(container), val(cli), val(_params)
        output:
          tuple val("${id}"), path(output), val(_params)
        stub:
          """
          # Adding NXF's `$moduleDir` to the path in order to resolve our own wrappers
          export PATH="${moduleDir}:\$PATH"
          STUB=1 $cli
          """
        script:
          def viash_temp = System.getenv("VIASH_TEMP") ?: "/tmp/"
          if (params.test)
            """
            # Some useful stuff
            export NUMBA_CACHE_DIR=/tmp/numba-cache
            # Running the pre-hook when necessary
            # Pass viash temp dir
            export VIASH_TEMP="${viash_temp}"
            # Adding NXF's `$moduleDir` to the path in order to resolve our own wrappers
            export PATH="./:${moduleDir}:\$PATH"
            ./${params.censor_dataset.tests.testScript} | tee $output
            """
          else
            """
            # Some useful stuff
            export NUMBA_CACHE_DIR=/tmp/numba-cache
            # Running the pre-hook when necessary
            # Pass viash temp dir
            export VIASH_TEMP="${viash_temp}"
            # Adding NXF's `$moduleDir` to the path in order to resolve our own wrappers
            export PATH="${moduleDir}:\$PATH"
            $cli
            """
      }
      
      workflow censor_dataset {
      
        take:
        id_input_params_
      
        main:
      
        def key = "censor_dataset"
      
        def id_input_output_function_cli_params_ =
          id_input_params_.map{ id, input, _params ->
      
            // Start from the (global) params and overwrite with the (local) _params
            def defaultParams = params[key] ? params[key] : [:]
            def overrideParams = _params[key] ? _params[key] : [:]
            def updtParams = defaultParams + overrideParams
            // Convert to List[Map] for the arguments
            def newParams = argumentsAsList(updtParams) + [ "id" : id ]
      
            // Generate output filenames, out comes a Map
            def output = outFromIn(newParams)
      
            // The process expects Path or List[Path], Maps need to be converted
            def inputsForProcess =
              (input in HashMap)
                ? input.collect{ k, v -> v }.flatten()
                : input
            def outputsForProcess = output.collect{ it.value }
      
            // For our machinery, we convert Path -> String in the input
            def inputs =
              (input in List || input in HashMap)
                ? (input in List)
                  ? input.collect{ it.name }
                  : input.collectEntries{ k, v -> [ k, (v in List) ? v.collect{it.name} : v.name ] }
                : input.name
            outputs = output.collectEntries{ [(it.name): it.value] }
      
            def finalParams = overrideIO(newParams, inputs, outputs)
      
            checkParams(finalParams)
      
            new Tuple6(
              id,
              inputsForProcess,
              outputsForProcess,
              effectiveContainer(finalParams),
              renderCLI([finalParams.command], finalParams.arguments),
              finalParams
            )
          }
      
        result_ = censor_dataset_process(id_input_output_function_cli_params_)
          | join(id_input_params_)
          | map{ id, output, _params, input, original_params ->
              def parsedOutput = _params.arguments
                .findAll{ it.type == "file" && it.direction == "Output" }
                .withIndex()
                .collectEntries{ it, i ->
                  // with one entry, output is of type Path and array selections
                  // would select just one element from the path
                  [(it.name): (output in List) ? output[i] : output ]
                }
              new Tuple3(id, parsedOutput, original_params)
            }
      
        result_
           | filter { it[1].keySet().size() > 1 }
           | view{">> Be careful, multiple outputs from this component!"}
      
        emit:
        result_.flatMap{ it ->
          (it[1].keySet().size() > 1)
            ? it[1].collect{ k, el -> [ it[0], [ (k): el ], it[2] ] }
            : it[1].collect{ k, el -> [ it[0], el, it[2] ] }
        }
      }
      
      workflow {
        def id = params.id
        def fname = "censor_dataset"
      
        def _params = params
      
        // could be refactored to be FP
        for (entry in params[fname].arguments) {
          def name = entry.value.name
          if (params[name] != null) {
            params[fname].arguments[name].value = params[name]
          }
        }
      
        def inputFiles = params.censor_dataset
          .arguments
          .findAll{ key, par -> par.type == "file" && par.direction == "Input" }
          .collectEntries{ key, par -> [(par.name): file(params[fname].arguments[par.name].value) ] }
      
        def ch_ = Channel.from("").map{ s -> new Tuple3(id, inputFiles, params)}
      
        result = censor_dataset(ch_)
      
        result \
          | filterOutput_train_mod1 \
          | view{ "Output for output_train_mod1: " + it[1] }
      
      
        result \
          | filterOutput_train_mod2 \
          | view{ "Output for output_train_mod2: " + it[1] }
      
      
        result \
          | filterOutput_train_sol \
          | view{ "Output for output_train_sol: " + it[1] }
      
      
        result \
          | filterOutput_test_mod1 \
          | view{ "Output for output_test_mod1: " + it[1] }
      
      
        result \
          | filterOutput_test_mod2 \
          | view{ "Output for output_test_mod2: " + it[1] }
      
      
        result \
          | filterOutput_test_sol \
          | view{ "Output for output_test_sol: " + it[1] }
      
      }
      
      // This workflow is not production-ready yet, we leave it in for future dev
      // TODO
      workflow test {
      
        take:
        rootDir
      
        main:
        params.test = true
        params.censor_dataset.output = "censor_dataset.log"
      
        Channel.from(rootDir) \
          | filter { params.censor_dataset.tests.isDefined } \
          | map{ p -> new Tuple3(
              "tests",
              params.censor_dataset.tests.testResources.collect{ file( p + it ) },
              params
          )} \
          | censor_dataset
      
        emit:
        censor_dataset.out
      }

    dest: "main.nf"
  description: "Censor an existing dataset: obfuscate gene names, remove cell identities\
    \ and\nshuffle cells of modalities, for distribution to competitors.\n"
  tests: []
  info: {}
platform:
  type: "nextflow"
  id: "nextflow"
  registry: "openproblems"
  namespace_separator: "_"
  publish: true
  separate_multiple_outputs: true
  labels:
  - "highmem"
  - "midtime"
  - "highcpu"
platforms: []
info:
  config: "src/match_modality/datasets/censor_dataset/config.vsh.yaml"
  platform: "nextflow"
  output: "target/nextflow/match_modality_datasets/censor_dataset"
  executable: "target/nextflow/match_modality_datasets/censor_dataset/censor_dataset"
  viash_version: "0.5.3"
  git_commit: "9a0dda117c40a01521f247b320fe4bf59806b6cb"
  git_remote: "https://github.com/openproblems-bio/neurips2021_multimodal_viash"
